{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"M0JNG-kkUoYh","executionInfo":{"status":"ok","timestamp":1681159228226,"user_tz":240,"elapsed":5689,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}}},"outputs":[],"source":["# Import modules\n","import pandas as pd\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","import torch\n","try:\n","  from transformers import BertTokenizer, BertModel\n","except:\n","  !pip install transformers\n","  from transformers import BertTokenizer, BertModel\n","\n","try:\n","  import langdetect\n","except:\n","  !pip install langdetect\n","  import langdetect"]},{"cell_type":"code","source":["# Stopwords\n","nltk.download('stopwords')\n","nltk_stopwords = set(stopwords.words('english'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WoRLtI5VQG-","executionInfo":{"status":"ok","timestamp":1681157298936,"user_tz":240,"elapsed":10,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}},"outputId":"fb34d232-87a1-4689-817a-b752743217b6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["# Functions\n","# Function to clean and process the sentence\n","def pre_process_sentence(sentence, nltk_stopwords):\n","    s = sentence.lower()\n","    s = s.split()\n","    words = []\n","    for w in s:\n","        w = w.strip(string.punctuation)\n","        if w not in nltk_stopwords and len(w) > 1:\n","            words.append(w)\n","    return ' '.join(words)\n","\n","# Function to clean the dataframe of non-English lyrics\n","def is_english(text):\n","    try:\n","        return langdetect.detect(text) == 'en'\n","    except:\n","        return False\n","\n","# Function to clean and process the dataframe\n","def pre_process_data(new_df, nltk_stopwords):\n","    new_df.dropna(subset=['text'], inplace=True)  # drop rows with missing values in the 'text' column\n","    new_df = new_df[new_df['text'].apply(is_english)]\n","    new_df.text = new_df.text.map(lambda x: pre_process_sentence(x, nltk_stopwords))\n","    new_df.reset_index(drop=True, inplace=True)\n","    return new_df\n"],"metadata":{"id":"N8oeZBpkVS8M","executionInfo":{"status":"ok","timestamp":1681157299190,"user_tz":240,"elapsed":259,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def get_bert_features(df, tokenizer, model, device, max_length=300, batch_size=32):\n","    encoded_corpus = tokenizer(text=df.text.tolist(),\n","                                add_special_tokens=True,\n","                                padding='max_length',\n","                                truncation='longest_first',\n","                                max_length=max_length,\n","                                return_attention_mask=True)\n","\n","    input_ids = encoded_corpus['input_ids']\n","    attention_mask = encoded_corpus['attention_mask']\n","\n","    # Filter long inputs\n","    def _filter_long_descriptions(tokenizer, descriptions, max_len):\n","        indices = []\n","        lengths = tokenizer(descriptions, padding=False, \n","                        truncation=False, return_length=True)['length']\n","        for i in range(len(descriptions)):\n","            if lengths[i] <= max_len-2:\n","                indices.append(i)\n","        return indices\n","\n","    short_descriptions = _filter_long_descriptions(tokenizer, \n","                                  df.text.tolist(), 300)\n","    input_ids = np.array(input_ids)[short_descriptions]\n","    attention_mask = np.array(attention_mask)[short_descriptions]\n","\n","    batch_size = 32\n","\n","    def _create_dataloaders(inputs, masks, batch_size):\n","        input_tensor = torch.tensor(inputs)\n","        mask_tensor = torch.tensor(masks)\n","        dataset = TensorDataset(input_tensor, mask_tensor)\n","        dataloader = DataLoader(dataset, batch_size=batch_size)\n","        return dataloader\n","\n","    tr_loader = _create_dataloaders(input_ids, attention_mask, batch_size)\n","\n","    # Put the model in evaluation mode\n","    model.eval()\n","\n","    # Define an empty list to store the features\n","    features_list = []\n","\n","    # Loop over the batches in the dataloader\n","    for batch in tr_loader:\n","        # Unpack the batch tuple into individual tensors\n","        input_ids = batch[0].to(device)\n","        attention_mask = batch[1].to(device)\n","\n","        # Pass the tensors through the model\n","        with torch.no_grad():\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        # Get the last hidden state from the BERT model\n","        last_hidden_state = outputs.last_hidden_state\n","\n","        # Append the features to the list\n","        features_list.append(last_hidden_state)\n","\n","    # Concatenate all of the features in the list\n","    all_features = torch.cat(features_list, dim=0)\n","\n","    # Return the concatenated tensor\n","    return all_features, short_descriptions\n"],"metadata":{"id":"Nt8UUgZXYe_i","executionInfo":{"status":"ok","timestamp":1681159228726,"user_tz":240,"elapsed":194,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Mounting for Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4A8tJaxVVoTA","executionInfo":{"status":"ok","timestamp":1681158418610,"user_tz":240,"elapsed":18818,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}},"outputId":"ac2d004a-54c8-4c93-baa3-d487d01e141b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Load data from CSV file\n","df = pd.read_csv('drive/MyDrive/BERT_Feature_Extraction/spotify_songs.csv')\n","\n","# Rename columns and assign to new dataframe\n","df = pd.DataFrame({\n","    'text': df.loc[:, 'lyrics'],\n","    'label': df.loc[:, 'track_popularity'],\n","    'genre': df.loc[:, 'playlist_genre']\n","})\n","\n","# Remove genres we aren't looking at\n","genres_to_keep = ['rock', 'pop', 'edm']\n","df = df[df['genre'].isin(genres_to_keep)].reset_index(drop=True)\n","\n","# Preprocess the dataframe\n","df = pre_process_data(df, nltk_stopwords)\n","\n","df.to_csv(f\"drive/MyDrive/BERT_Feature_Extraction/processed_spotify_data.csv\", index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UzfBxfLqVXRx","executionInfo":{"status":"ok","timestamp":1681157632102,"user_tz":240,"elapsed":99788,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}},"outputId":"eabf78c0-21ab-4f6a-ec0f-b377090577c5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-fe6234b9d23a>:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  new_df.text = new_df.text.map(lambda x: pre_process_sentence(x, nltk_stopwords))\n"]}]},{"cell_type":"code","source":["# Use BERT for feature extraction\n","\n","df = pd.read_csv('drive/MyDrive/BERT_Feature_Extraction/processed_spotify_data.csv')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n","\n","if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(\"Using GPU.\")\n","else:\n","    print(\"No GPU available, using the CPU instead.\")\n","    device = torch.device(\"cpu\")\n","model.to(device)\n","\n","features, short_descriptions = get_bert_features(df, tokenizer, model, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWs38fQeVcOa","executionInfo":{"status":"ok","timestamp":1681159462663,"user_tz":240,"elapsed":229017,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}},"outputId":"e7fdbc91-b761-4a5e-c528-d16aa4967920"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Using GPU.\n"]},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["#df = df.iloc[short_descriptions]\n","array_of_features = features.cpu().numpy()\n","df['BERT_features'] = list(array_of_features)\n","\n"],"metadata":{"id":"5qh4wlNL19m0","executionInfo":{"status":"ok","timestamp":1681159762324,"user_tz":240,"elapsed":6056,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["df['genre'].unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYBV63jH7kfK","executionInfo":{"status":"ok","timestamp":1681159889647,"user_tz":240,"elapsed":161,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}},"outputId":"33caf3db-2968-46b6-f30d-da3a3072e7d8"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['rock', 'edm', 'pop'], dtype=object)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Drop columns\n","#df.drop(columns=['text'], inplace=True)\n","genres_to_keep = ['pop', 'edm', 'rock']\n","# Save the datasets to their respective genres\n","for genre in genres_to_keep:\n","    df_filtered = df[df['genre'] == genre].reset_index(drop=True)\n","    df_filtered = df_filtered.drop('genre', axis=1)\n","    df_filtered.to_csv(f\"drive/MyDrive/BERT_Feature_Extraction/{genre}_processed_spotify_data.csv\", index=False)\n"],"metadata":{"id":"-Xlo_P9PVhbc","executionInfo":{"status":"ok","timestamp":1681159967733,"user_tz":240,"elapsed":2998,"user":{"displayName":"Luke Boll","userId":"02641970037418082853"}}},"execution_count":16,"outputs":[]}]}